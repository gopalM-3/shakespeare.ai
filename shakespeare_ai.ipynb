{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# shakespeare.ai\n",
        "\n",
        "A fun text generator built using a transformer, to generate text similar to William Shakespeare's style of English."
      ],
      "metadata": {
        "id": "uySxXIcDBlYv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "torch.manual_seed(1618)"
      ],
      "metadata": {
        "id": "WY3chwO25SQO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell imports the necessary modules from the PyTorch library to work with neural networks. The `torch` module is the main package for tensor computations, while `torch.nn` provides support for defining and training neural networks. The `torch.nn.functional` module contains various functions for implementing neural network operations.\n",
        "\n",
        "Additionally, the line `torch.manual_seed(1618)` sets the random seed to 1618. Setting a random seed ensures reproducibility, meaning that the random values generated by PyTorch will be the same each time you run the code, which is useful for debugging and result reproducibility purposes."
      ],
      "metadata": {
        "id": "OH5iI0GLBfZ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# hyperparameters\n",
        "batchSize = 64\n",
        "blockSize = 256\n",
        "maxIters = 5000\n",
        "evalInter = 500\n",
        "learningRate = 3e-4\n",
        "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "evalIters = 200\n",
        "nEmbd = 384\n",
        "nHead = 6\n",
        "nLayer = 6\n",
        "dropout = 0.2"
      ],
      "metadata": {
        "id": "jDdwlSEq5U9U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this cell, various hyperparameters for the model are defined. These hyperparameters control different aspects of the training and architecture of the neural network. Here's a brief explanation of each hyperparameter:\n",
        "\n",
        "- `batchSize`: The number of samples in each training batch.\n",
        "- `blockSize`: The length of the input text sequence used for training.\n",
        "- `maxIters`: The maximum number of training iterations.\n",
        "- `evalInter`: The interval at which to evaluate the model during training.\n",
        "- `learningRate`: The learning rate used in the optimization algorithm.\n",
        "- `evalIters`: The number of evaluation iterations.\n",
        "- `nEmbd`: The dimensionality of the embedding layer.\n",
        "- `nHead`: The number of attention heads in the transformer model.\n",
        "- `nLayer`: The number of layers in the transformer model.\n",
        "- `dropout`: The probability of dropout to apply during training for regularization.\n",
        "\n",
        "Feel free to adjust these hyperparameters according to your specific requirements and the characteristics of your dataset. This particular setting accounts to about 10 million parameters, I'd suggest not running it on CPU."
      ],
      "metadata": {
        "id": "3FZyOek4CSZq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# reading and viewing the text corpus\n",
        "with open(\"/content/drive/MyDrive/shakespeare.txt\", \"r\") as file:\n",
        "    text = file.read()\n",
        "\n",
        "print(f\"Length of the dataset: {len(text)}\")\n",
        "print(\"First 1000 characters:\")\n",
        "print(text[:1000])"
      ],
      "metadata": {
        "id": "rF9QF37w5cW3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The cell reads a text corpus from a file named \"shakespeare.txt\" and stores its content in a variable called `text`. The `with open()` statement ensures proper handling of file resources by automatically closing the file once it's done being read.\n",
        "\n",
        "The length of the text corpus is then printed using the `len()` function, providing an indication of the total number of characters in the dataset.\n",
        "\n",
        "Lastly, the first 1000 characters of the corpus are printed using `print(text[:1000])`, allowing a glimpse into the content of the dataset.\n",
        "\n",
        "This code is useful for loading and examining the text corpus before further processing or training. It helps ensure that the data is correctly loaded and provides an initial understanding of the dataset's structure and content."
      ],
      "metadata": {
        "id": "HpZ861vFCtoc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# all the chars that occur in the doc\n",
        "chars = sorted(list(set(text)))\n",
        "vocabSize = len(chars)\n",
        "print(\"Charset:\", \"\".join(chars))\n",
        "print(f\"{vocabSize=}\")\n",
        "\n",
        "# creating mappings from chars to ints\n",
        "stoi = {ch: i for i, ch in enumerate(chars)}\n",
        "itos = {i: ch for i, ch in enumerate(chars)}"
      ],
      "metadata": {
        "id": "IlEPdCno5fw2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The cell performs the following tasks:\n",
        "\n",
        "1. It creates a sorted list of unique characters present in the `text` corpus using `set(text)` to eliminate duplicates and `sorted()` to sort them in ascending order. The resulting list is assigned to the variable `chars`.\n",
        "2. The variable `vocabSize` is then assigned the length of the `chars` list, representing the total number of unique characters in the corpus.\n",
        "3. The line `print(\"Charset:\", \"\".join(chars))` outputs the sorted list of characters as a string, displaying all the unique characters present in the dataset.\n",
        "4. The line `print(f\"{vocabSize=}\")` prints the value of `vocabSize`, which represents the number of unique characters in the dataset.\n",
        "\n",
        "The next part of the code snippet involves creating two mappings: `stoi` (string to integer) and `itos` (integer to string). These mappings are dictionaries that associate each character with a unique index or vice versa.\n",
        "\n",
        "- The line `stoi = {ch: i for i, ch in enumerate(chars)}` creates the `stoi` dictionary, where each character from `chars` is mapped to its corresponding index.\n",
        "- The line `itos = {i: ch for i, ch in enumerate(chars)}` creates the `itos` dictionary, where each index is mapped to its corresponding character.\n",
        "\n",
        "These mappings are useful for encoding and decoding characters during text generation or any other tasks that require converting between characters and their corresponding integer representations."
      ],
      "metadata": {
        "id": "qIMf9KloCwBB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# encoder function; string to list(int)\n",
        "def encode(s):\n",
        "    return [stoi[c] for c in s]\n",
        "\n",
        "# decoder function; list(int) to string\n",
        "def decode(l):\n",
        "    return \"\".join(itos[i] for i in l)"
      ],
      "metadata": {
        "id": "hijIDUC_5lYd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The cell defines two functions:\n",
        "\n",
        "1. `encode(s)`: This function takes a string `s` as input and returns a list of integers representing the encoded version of the string. The function utilizes a list comprehension and the `stoi` dictionary to map each character in `s` to its corresponding integer index.\n",
        "\n",
        "2. `decode(l)`: This function takes a list of integers `l` as input and returns a string representing the decoded version of the list. It uses a list comprehension and the `itos` dictionary to map each integer in `l` to its corresponding character and then joins the characters to form a string.\n",
        "\n",
        "These functions are useful for converting between string and integer representations of text. The `encode()` function is typically used to convert input text into a numerical representation that can be fed into a neural network, while the `decode()` function is used to convert the output of a neural network (a list of predicted integers) back into a readable text format."
      ],
      "metadata": {
        "id": "O7IathujC8cK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"***Test***\")\n",
        "temp = encode(\"Carpe diem!\")\n",
        "print(\"Encoded vector for 'Carpe diem!':\", temp)\n",
        "print(decode(temp))\n",
        "\n",
        "# encoding the entire doc\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "print(f\"Data encoding metadata: {data.shape=}, {data.dtype=}\")"
      ],
      "metadata": {
        "id": "duALVL6P5pK6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this cell, a test case is performed to demonstrate the usage of the `encode()` and `decode()` functions.\n",
        "\n",
        "- The line `temp = encode(\"Carpe diem!\")` encodes the string \"Carpe diem!\" using the `encode()` function, resulting in a list of integers representing the encoded version of the text.\n",
        "- The line `print(\"Encoded vector for 'Carpe diem!':\", temp)` prints the encoded vector.\n",
        "- The line `print(decode(temp))` decodes the encoded vector using the `decode()` function and prints the decoded string.\n",
        "\n",
        "After the test case, the entire `text` corpus is encoded using `encode(text)`, and the resulting list of integers is converted into a `torch.tensor` using `torch.tensor(encode(text), dtype=torch.long)`. The resulting `data` tensor represents the encoded version of the entire text corpus.\n",
        "\n",
        "The last line `print(f\"Data encoding metadata: {data.shape=}, {data.dtype=}\")` outputs the shape and data type information of the `data` tensor, providing metadata about the encoded data."
      ],
      "metadata": {
        "id": "OnVla0-NDYZy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train & val split\n",
        "n = int(0.9*len(data))\n",
        "train = data[:n]\n",
        "val = data[n:]"
      ],
      "metadata": {
        "id": "4Ii1cLNY5qWb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The cell performs a train-validation split on the encoded data.\n",
        "\n",
        "The encoded data, stored in the `data` tensor, is split into two parts: a training set (`train`) and a validation set (`val`). The split is done by determining the index `n` which represents 90% of the data length.\n",
        "\n",
        "- `train` is created by taking the portion of the `data` tensor from the beginning up to index `n`, representing the first 90% of the data.\n",
        "- `val` is created by taking the portion of the `data` tensor from index `n` until the end, representing the remaining 10% of the data.\n",
        "\n",
        "This train-validation split is commonly used in machine learning to divide the data into two separate sets for training and evaluating a model, respectively. Adjusting the split percentage allows for different proportions of data allocated for training and validation."
      ],
      "metadata": {
        "id": "uV1LTHWPDojf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# loading data\n",
        "def getBatch(split):\n",
        "    data = train if split == \"train\" else val\n",
        "    idx = torch.randint(len(data) - blockSize, (batchSize,))\n",
        "    x = torch.stack([data[i: i + blockSize] for i in idx])\n",
        "    y = torch.stack([data[i + 1: i + blockSize + 1] for i in idx])\n",
        "    # x, y = x.to(device), y.to(device)\n",
        "\n",
        "    return x, y"
      ],
      "metadata": {
        "id": "SWxsTA3w5syC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `getBatch` function retrieves a batch of data for either the training or validation split.\n",
        "\n",
        "- `getBatch(split)` is a function that takes a `split` argument indicating whether to fetch a batch for the \"train\" or \"val\" (validation) split.\n",
        "- Inside the function, the `data` variable is assigned the `train` tensor if the `split` is \"train\", and the `val` tensor otherwise.\n",
        "- Random indices `idx` are generated to select samples from `data`. The indices are generated within the valid range of indices that allow constructing input and target sequences of size `blockSize`.\n",
        "- The input sequences `x` are constructed by selecting slices of length `blockSize` from `data` based on the generated indices `idx`.\n",
        "- The target sequences `y` are constructed similarly to `x`, but with an offset of one.\n",
        "- Finally, the function returns the input sequences `x` and the corresponding target sequences `y`.\n",
        "\n",
        "Note: The commented line `# x, y = x.to(device), y.to(device)` suggests that the code is originally written to run on a specific device, such as a CUDA-capable GPU. However, it is currently disabled. If desired, uncomment and modify this line to specify the device for tensor computations."
      ],
      "metadata": {
        "id": "RDSkpQQAEBrS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# creating necessary classes\n",
        "\n",
        "class Head(nn.Module):\n",
        "    # one head of self-attention\n",
        "\n",
        "    def __init__(self, headSize):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(nEmbd, headSize, bias=False)\n",
        "        self.query = nn.Linear(nEmbd, headSize, bias=False)\n",
        "        self.value = nn.Linear(nEmbd, headSize, bias=False)\n",
        "        self.register_buffer(\"tril\", torch.tril(\n",
        "            torch.ones(blockSize, blockSize)))\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.shape\n",
        "        k = self.key(x)  # (B, T, C)\n",
        "        q = self.query(x)  # (B, T, C)\n",
        "\n",
        "        # computing attention weights or affinities\n",
        "        wei = q @ k.transpose(-2, -1) * C**(-0.5)\n",
        "        # (B, T, 16) @ (B, 16, T) --> (B, T, T)\n",
        "\n",
        "        wei = wei.masked_fill(\n",
        "            self.tril[:T, :T] == 0, float(\"-inf\"))  # (B, T, T)\n",
        "        '''\n",
        "        this is a decoder block, it prevents token communication with the future tokens\n",
        "        if this decoder block is absent, the tokens will be able to communicate with the past & future tokens\n",
        "        '''\n",
        "\n",
        "        wei = F.softmax(wei, dim=-1)\n",
        "        wei = self.dropout(wei)\n",
        "\n",
        "        # computing weighted sum of the values\n",
        "        v = self.value(x)\n",
        "        out = wei @ v\n",
        "\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    # multiple heads of self-attention in parallel\n",
        "\n",
        "    def __init__(self, numHeads, headSize):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(headSize)\n",
        "                                    for _ in range(0, numHeads)])\n",
        "        self.proj = nn.Linear(nEmbd, nEmbd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.proj(out)\n",
        "        out = self.dropout(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    # simple linear layer followed by non-linearity\n",
        "\n",
        "    def __init__(self, nEmbd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(nEmbd, 4 * nEmbd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * nEmbd, nEmbd),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    # transformer block: communication followed by computation\n",
        "\n",
        "    def __init__(self, nEmbd, nHead):\n",
        "        # nEmbd: embedding dimension, nHead: number of heads we'd like\n",
        "        super().__init__()\n",
        "        headSize = nEmbd // nHead\n",
        "        # nHead heads of headSize dimensional self-attention\n",
        "        self.sa = MultiHeadAttention(nHead, headSize)\n",
        "        self.ffwd = FeedForward(nEmbd)\n",
        "        self.ln1 = nn.LayerNorm(nEmbd)  # layer norm 1\n",
        "        self.ln2 = nn.LayerNorm(nEmbd)  # layer norm 2\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "    # simple bigram language model\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads the logits of the next token\n",
        "        self.tokenEmbeddingTable = nn.Embedding(vocabSize, nEmbd)\n",
        "        self.positionEmbeddingTable = nn.Embedding(blockSize, nEmbd)\n",
        "        self.blocks = nn.Sequential(\n",
        "            *[Block(nEmbd, nHead=nHead) for _ in range(0, nLayer)])\n",
        "        self.lnF = nn.LayerNorm(nEmbd, nEmbd)  # final layer norm\n",
        "        self.lmHead = nn.Linear(nEmbd, vocabSize)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        # idx & targets are both (B, T) tensors of integers\n",
        "        B, T = idx.shape\n",
        "\n",
        "        tokEmb = self.tokenEmbeddingTable(idx)  # (B, T, C)\n",
        "        posEmb = self.positionEmbeddingTable(\n",
        "            torch.arange(T))  # (T, C)  ''', device=device'''\n",
        "        x = tokEmb + posEmb  # (B, T, C)\n",
        "        x = self.blocks(x)  # multi head attention, (B, T, C)\n",
        "        x = self.lnF(x)  # (B, T, C)\n",
        "        logits = self.lmHead(x)  # (B, T, C=len(vocabSize))\n",
        "\n",
        "        if targets == None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    # even though the model was built with the vision to use context from 1 to blockSize, this is sampling using bigrams\n",
        "    def generate(self, idx, maxNewTokens):\n",
        "        # idx is a (B, T) array of indices of the current context\n",
        "        for _ in range(0, maxNewTokens):\n",
        "            # cropping idx to the last blockSize tokens\n",
        "            idxCond = idx[:, -blockSize:]\n",
        "            # getting the predictions\n",
        "            logits, loss = self(idxCond)\n",
        "            # focusing only on the last time step (essentially making it a bigram model)\n",
        "            logits = logits[:, -1, :]\n",
        "            # applying softmax to get probablities\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            # sampling from a multinomial distribution\n",
        "            idxNext = torch.multinomial(probs, num_samples=1)\n",
        "            # appending the predicted index to a running vector\n",
        "            idx = torch.cat((idx, idxNext), dim=1)\n",
        "\n",
        "        return idx"
      ],
      "metadata": {
        "id": "IcX7emqz6Bo3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The cell defines several classes necessary for the language model:\n",
        "\n",
        "1. `Head`: Represents one head of self-attention.\n",
        "2. `MultiHeadAttention`: Contains multiple heads of self-attention in parallel.\n",
        "3. `FeedForward`: Implements a simple linear layer followed by a non-linearity.\n",
        "4. `Block`: Represents a transformer block, which consists of communication and computation.\n",
        "5. `BigramLanguageModel`: Represents a simple bigram language model.\n",
        "\n",
        "Each class has a `forward` method that defines the forward pass of the model. Here's a brief summary of each class:\n",
        "\n",
        "- `Head`:\n",
        "  - `__init__(self, headSize)`: Initializes the head with linear layers for key, query, and value, as well as dropout and a buffer for the lower triangular mask.\n",
        "  - `forward(self, x)`: Computes self-attention using key, query, and value and returns the output.\n",
        "\n",
        "- `MultiHeadAttention`:\n",
        "  - `__init__(self, numHeads, headSize)`: Initializes multiple heads of self-attention with the specified number of heads and head size.\n",
        "  - `forward(self, x)`: Applies each head of self-attention in parallel and returns the concatenated output.\n",
        "\n",
        "- `FeedForward`:\n",
        "  - `__init__(self, nEmbd)`: Initializes a simple feed-forward module with linear layers and dropout.\n",
        "  - `forward(self, x)`: Passes the input through the linear layers and returns the output.\n",
        "\n",
        "- `Block`:\n",
        "  - `__init__(self, nEmbd, nHead)`: Initializes a transformer block with self-attention and feed-forward layers.\n",
        "  - `forward(self, x)`: Performs self-attention and feed-forward computations on the input and returns the output.\n",
        "\n",
        "- `BigramLanguageModel`:\n",
        "  - `__init__(self)`: Initializes the bigram language model with embedding tables, transformer blocks, and linear layers.\n",
        "  - `forward(self, idx, targets=None)`: Computes the forward pass of the model, returning logits and an optional loss if targets are provided.\n",
        "  - `generate(self, idx, maxNewTokens)`: Generates new tokens based on the given input indices using the model.\n",
        "\n",
        "These classes provide the necessary building blocks for the language model architecture, including self-attention, feed-forward layers, and transformer blocks."
      ],
      "metadata": {
        "id": "FEwVINJtEkGB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "xb, yb = getBatch(\"train\")\n",
        "# the below code is to analyse & interpret the inputs and targets\n",
        "# print(f\"***Inputs***\\nShape: {xb.shape}\\n{xb}\")\n",
        "# print(f\"***Targets***\\nShape: {yb.shape}\\n{yb}\")\n",
        "\n",
        "# for b in range(batchSize):\n",
        "#     for t in range(blockSize):\n",
        "#         context = xb[b, : t + 1]\n",
        "#         target = yb[b, t]\n",
        "#         print(f\"when input is {context.tolist()} the target: {target}\")"
      ],
      "metadata": {
        "id": "N_yF-ylZ6Fmg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The cell defines a snippet that generates a batch of inputs and targets using the `getBatch` function. It includes commented-out code that can be used to analyze and interpret the inputs and targets.\n",
        "\n",
        "The code initializes `xb` and `yb` by calling `getBatch` with the argument \"train\" to obtain a batch of inputs and targets for training.\n",
        "\n",
        "The commented-out code provides an example of how to analyze and interpret the inputs and targets. It shows how to print the shape and contents of the inputs and targets. Additionally, it includes nested loops to iterate over each element in the batch and timestep, printing the context (input sequence) and target for each timestep.\n",
        "\n",
        "This code can be useful for understanding the structure and content of the inputs and targets in the training batch."
      ],
      "metadata": {
        "id": "bHwpc9eiFJbU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = BigramLanguageModel()\n",
        "# m = model.to(device)"
      ],
      "metadata": {
        "id": "eX0yFhBo6JK9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The cell creates an instance of the `BigramLanguageModel` class and assigns it to the variable `model`. There is a commented-out line that suggests the model might be moved to a device, but it is currently disabled.\n",
        "\n",
        "The `BigramLanguageModel` is a class representing a simple bigram language model. The instantiated `model` variable represents an instance of this model. The commented-out line suggests that the model could be moved to a specific device using the `to` method, where `device` is a variable that holds the target device (e.g., \"cuda\" for GPU or \"cpu\" for CPU). However, in the provided code, this line is currently disabled by being commented out."
      ],
      "metadata": {
        "id": "vAgNUle3FRwV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# function for estimating losses\n",
        "@torch.no_grad()\n",
        "def estimateLoss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in [\"train\", \"val\"]:\n",
        "        losses = torch.zeros(evalIters)\n",
        "        for k in range(0, evalIters):\n",
        "            X, y = getBatch(split)\n",
        "            logits, loss = model(X, y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "\n",
        "    return out"
      ],
      "metadata": {
        "id": "IBme3JTb5v6A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The cell defines a function named `estimateLoss` for estimating losses during evaluation.\n",
        "\n",
        "The function is decorated with `@torch.no_grad()` to ensure that no gradients are computed during the evaluation.\n",
        "\n",
        "Inside the function:\n",
        "- The dictionary `out` is initialized to store the loss values.\n",
        "- The model is set to evaluation mode using `model.eval()`.\n",
        "- For each split (either \"train\" or \"val\"):\n",
        "  - A tensor `losses` is created to store the losses for the current split.\n",
        "  - For `evalIters` number of iterations:\n",
        "    - Input sequences `X` and target sequences `y` are obtained using the `getBatch` function.\n",
        "    - The model is called with `X` and `y` to obtain logits and loss.\n",
        "    - The loss value is extracted using `loss.item()` and stored in the `losses` tensor.\n",
        "  - The mean of the `losses` tensor is calculated and assigned to the `split` key in the `out` dictionary.\n",
        "- Finally, the model is set back to train mode using `model.train()`, and the `out` dictionary containing the mean losses for each split is returned.\n",
        "\n",
        "This function is useful for estimating losses during evaluation, providing insights into the performance of the model on the training and validation splits."
      ],
      "metadata": {
        "id": "wp9MOtdjELlo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# preliminary analysis before training\n",
        "print(\"***Preliminary analysis***\")\n",
        "logits, loss = model(xb, yb)\n",
        "print(f\"{logits.shape=}, {loss=}\")  # expected loss = -ln(1/65) = -4.17\n",
        "\n",
        "print(decode(model.generate(idx=torch.zeros(\n",
        "    (1, 1), dtype=torch.long), maxNewTokens=100)[0].tolist()))"
      ],
      "metadata": {
        "id": "9rXIB2b_6LNp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The cell performs preliminary analysis before training the `model`. It includes two print statements to analyze the shape of the `logits` tensor and the value of the `loss` variable. Additionally, it generates a sequence of text using the `generate` method of the `model` and prints the decoded text.\n",
        "\n",
        "The first print statement displays the shape of the `logits` tensor and the value of the `loss` variable. It helps in understanding the dimensions of the `logits` tensor and the initial value of the loss.\n",
        "\n",
        "The second print statement generates a sequence of text by calling the `generate` method of the `model`. It initializes the generation process with an input tensor of shape `(1, 1)` filled with zeros and generates a sequence of maximum length `100`. The generated sequence is then decoded using the `decode` function and printed. This provides a glimpse of the text that the model might produce during generation."
      ],
      "metadata": {
        "id": "xNHwBktEFaV1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"***Training begins***\")\n",
        "\n",
        "# creating an optimizer object\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
        "\n",
        "for iter in range(0, maxIters):\n",
        "    # evaluating train & val loss once in a while\n",
        "    if iter % evalInter == 0:\n",
        "        losses = estimateLoss()\n",
        "        print(\n",
        "            f\"{iter=}: train loss - {losses['train']:.4f}, val loss - {losses['val']:.4f}\")\n",
        "\n",
        "    # sampling a batch of data\n",
        "    xb, yb = getBatch(\"train\")\n",
        "\n",
        "    # evaluating the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ],
      "metadata": {
        "id": "_9gEOv5n6OEM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The cell starts the training process for the model. It includes a loop that iterates over a specified number of iterations (`maxIters`). Within each iteration, it performs the following steps:\n",
        "\n",
        "1. Every `evalInter` iterations, it calls the `estimateLoss` function to evaluate the training and validation loss. The current iteration number, training loss, and validation loss are printed.\n",
        "2. It obtains a batch of training data by calling the `getBatch` function.\n",
        "3. It calculates the logits and loss by calling the `model` with the input batch (`xb`) and target batch (`yb`).\n",
        "4. It initializes the gradients of the model parameters with respect to the loss using `optimizer.zero_grad(set_to_none=True)`.\n",
        "5. It performs backpropagation by calling `loss.backward()` to compute the gradients.\n",
        "6. It updates the model parameters by calling `optimizer.step()` to perform an optimization step.\n",
        "\n",
        "The code initializes an optimizer object (`AdamW`) to optimize the model parameters. Then, in each iteration, it performs training steps, evaluates the loss periodically, and updates the model parameters using backpropagation and optimization."
      ],
      "metadata": {
        "id": "SGXLdxHwFkMw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# sampling from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long) # ''', device=device'''\n",
        "print(decode(model.generate(context, maxNewTokens=100)[0].tolist()))"
      ],
      "metadata": {
        "id": "aaaff9IzxbO9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code cell generates text samples from the trained model. It initializes a context tensor with shape (1, 1) containing zeros, and then calls the `generate` method of the `model` to generate text based on this context. The generated text has a maximum length of 100 tokens.\n",
        "\n",
        "The generated text is then printed."
      ],
      "metadata": {
        "id": "rNa7a9euFrUK"
      }
    }
  ]
}